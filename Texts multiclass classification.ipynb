{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass classification of texts - small descriptions of\n",
    "# tender purchases in Russian;\n",
    "\n",
    "# train.csv contains 'index', 'proc_name' (text description in Russian) and\n",
    "# 'target' - ground-truth allocation to classes;\n",
    "\n",
    "# test.csv contains only 'index' and 'proc_name'.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import utils\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t_path = 'C:/some_folder/'\n",
    "path_train = t_path + 'train.csv'\n",
    "path_test = t_path + 'test.csv'\n",
    "result_path = t_path + 'result.csv'\n",
    "\n",
    "rs = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading the dataframes and quickly looking at the data\n",
    "train_df = pd.read_csv(path_train, sep=',')\n",
    "test_df = pd.read_csv(path_test, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NAN values\n",
    "for col in train_df.columns:\n",
    "    s = train_df[col].isnull()==True\n",
    "    print('na values in ' + col + '...' + str(sum(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning\n",
    "def clean_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Substituting line breaks to spaces\n",
    "    text = re.sub(\"^\\s+|\\n|\\r|\\s+$\", ' ', text)\n",
    "    \n",
    "    # Deleting numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Substituting punctuation (then deleting it)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Deleting extra spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function to both dataframes\n",
    "train_df['proc_name'] = train_df.apply(lambda x: clean_text(x['proc_name']), axis=1)\n",
    "test_df['proc_name'] = test_df.apply(lambda x: clean_text(x['proc_name']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing with the help of Pymystem3\n",
    "\n",
    "# Trying to minimize Mystem() calls - that's why creating a big string out of a series;\n",
    "# Otherwise it works too slow\n",
    "\n",
    "def lemmatize_column(df, col_name):\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    m = Mystem()\n",
    "\n",
    "\n",
    "    a = list(df[col_name])\n",
    "    b = '<<>>'.join(a)\n",
    "    c = m.lemmatize(b)\n",
    "    d = ''.join(c)\n",
    "    d = ' '.join(d.split())\n",
    "    df[col_name + '_lemma'] = d.split('<<>>')\n",
    "\n",
    "    df.drop([col_name], axis=1, inplace=True)\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = lemmatize_column(train_df, 'proc_name')\n",
    "test_df = lemmatize_column(test_df, 'proc_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NAN after lemmatization\n",
    "s = train_df['proc_name_lemma'].isnull()==True\n",
    "train_df.loc[s, 'proc_name_lemma'] = 'unknown'\n",
    "\n",
    "s = test_df['proc_name_lemma'].isnull()==True\n",
    "test_df.loc[s, 'proc_name_lemma'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stopwords = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop-words (Nltk library)\n",
    "def remove_russian_stopwords(text):\n",
    "    a = text.split(' ')\n",
    "    a = [item for item in a if item not in russian_stopwords]\n",
    "    return ' '.join(a)\n",
    "\n",
    "train_df['proc_name_lemma'] = train_df.apply(lambda x: remove_russian_stopwords(x['proc_name_lemma']), axis=1)\n",
    "test_df['proc_name_lemma'] = test_df.apply(lambda x: remove_russian_stopwords(x['proc_name_lemma']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First classifier based of CountVectorizer + Tf-Idf + Naive Bayes \n",
    "\n",
    "# Mixing the dataframe\n",
    "train_df = train_df.sample(n=len(train_df), random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['proc_name_lemma']\n",
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    cnt += 1\n",
    "    \n",
    "\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Setting max_df parameter to avoid words common to all descriptions\n",
    "    count_vect = CountVectorizer(max_df=0.2)\n",
    "    X_train = count_vect.fit_transform(X_train)\n",
    "    X_test = count_vect.transform(X_test)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train = tfidf_transformer.fit_transform(X_train)\n",
    "    X_test = tfidf_transformer.transform(X_test)\n",
    "    \n",
    "    NB_classifier = MultinomialNB().fit(X_train, y_train)\n",
    "    predicted = NB_classifier.predict(X_test)\n",
    "    print('(NB) Fold ' + str(cnt) + ', accuracy: ... ' + str(np.mean(predicted == y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another model: creating features with Gensim doc2vec,\n",
    "# then passing it to Logistic regression classifier\n",
    "\n",
    "# Delayed sample\n",
    "train, test = train_test_split(train_df, test_size=0.33, random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors\n",
    "\n",
    "# Preparing the data, performing doc2vec learning\n",
    "def get_doc_2_vec_data(train_df, test_df):\n",
    "    \n",
    "    train_tagged = train_df.apply(\n",
    "        lambda x: TaggedDocument(words=x['proc_name_lemma'].split(), tags=[x['target']]), axis=1)\n",
    "    \n",
    "    # 'target' for train sample, 'index' for test sample\n",
    "    if 'target' in test_df.columns:\n",
    "        t_col = 'target'\n",
    "    else:\n",
    "        t_col = 'index'\n",
    "    \n",
    "    test_tagged = test_df.apply(\n",
    "        lambda x: TaggedDocument(words=x['proc_name_lemma'].split(), tags=[x[t_col]]), axis=1)\n",
    "    \n",
    "    print(train_tagged.values[0])\n",
    "    \n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=200, negative=5, hs=0, min_count=2, window=15)\n",
    "    model_dbow.random.seed(rs)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n",
    "    \n",
    "    for epoch in range(30):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "        \n",
    "    y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "    y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "    \n",
    "    return y_train, X_train, y_test, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train, y_test, X_test = get_doc_2_vec_data(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the logistic regression\n",
    "\n",
    "multinomial_lr = LogisticRegression(multi_class='multinomial', solver='newton-cg', random_state=rs).fit(X_train, y_train)\n",
    "predictions = multinomial_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy (Logistic Regression) - doc2vec features:')\n",
    "print(str(np.mean(predictions==test['target'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model based on Logistic regression (its' validation results are better)\n",
    "\n",
    "train_df.sort_values(['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train, y_test, X_test = get_doc_2_vec_data(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial_lr = LogisticRegression(multi_class='multinomial', solver='newton-cg', random_state=rs).fit(X_train, y_train)\n",
    "predictions = multinomial_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results to 'result.csv' file\n",
    "\n",
    "test_df['target'] = predictions\n",
    "\n",
    "test_df.drop(['proc_name_lemma'], axis=1, inplace=True)\n",
    "\n",
    "test_df.to_csv(result_path, sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
