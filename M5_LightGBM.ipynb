{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Code recorded by Artem Remezov\n\n# One of my solutions for 'M5 Forecasting - Accuracy' competition\n# (https://www.kaggle.com/c/m5-forecasting-accuracy)\n# based on gradient boosting model\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from math import sqrt\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom datetime import date, timedelta\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport itertools\nimport matplotlib.pyplot as plt\nimport os\nimport matplotlib\nfrom typing import Union\nfrom tqdm.auto import tqdm as tqdm\nfrom sklearn.linear_model import LinearRegression\nfrom math import isnan as math_isnan\nimport math, decimal\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nimport lightgbm as lgb\nfrom IPython.display import FileLink\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Credits to multiple Kaggle posts\ndef reduce_mem_usage(df, verbose=False):\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    int_columns = df.select_dtypes(include=[\"int\"]).columns\n    float_columns = df.select_dtypes(include=[\"float\"]).columns\n\n    for col in int_columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n\n    for col in float_columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df\n\n\n# Credits to: https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/154776                        \ndef get_moon_phase(d):  # 0=new, 4=full; 4 days/phase\n    dec = decimal.Decimal\n    diff = d - datetime(2001, 1, 1)\n    days = dec(diff.days) + (dec(diff.seconds) / dec(86400))\n    lunations = dec(\"0.20439731\") + (days * dec(\"0.03386319269\"))\n    phase_index = math.floor((lunations % dec(1) * dec(8)) + dec('0.5'))\n    return int(phase_index) & 7                        \n\n\n# Credits to: https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python\ndef encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    res = res.drop([feature_to_encode], axis=1)\n    return(res) \n\n\ndef process_calendar():\n    \n    print('Start processing calendar ... ' + str(datetime.now()))\n    \n    # Converting 'date' to a proper date format\n    calendar['date'] =  pd.to_datetime(calendar['date'], format='%Y-%m-%d')\n\n\n    # Black Friday (https://www.timeanddate.com/holidays/us/black-friday)\n    black_friday_dates = [\"2011-11-25\", \"2012-11-23\", \"2013-11-29\", \"2014-11-28\", \"2015-11-27\"]\n\n    for item in black_friday_dates:\n        s = calendar['date']==item\n        calendar.loc[s, 'black_friday'] = 1\n\n    s = calendar['black_friday'].isnull()==True\n    calendar.loc[s, 'black_friday'] = 0       \n\n\n    # Dates of Ramadan\n    Ramadan_dates = [['2011-08-01', '2011-08-30'], \n                     ['2012-07-20', '2012-08-18'], \n                     ['2013-07-09', '2013-08-07'], \n                     ['2014-06-29', '2014-07-27'], \n                     ['2015-06-18', '2015-07-17'], \n                     ['2016-06-07', '2016-07-06']]\n\n    for item in Ramadan_dates:\n        s = (calendar['date'] >= item[0]) & (calendar['date'] <= item[1])\n        calendar.loc[s, 'Ramadan'] = 1\n\n    s = calendar['Ramadan'].isnull()==True\n    calendar.loc[s, 'Ramadan'] = 0\n\n\n\n    # Processing default fields from the calendar\n    for item in ['Religous', 'National', 'Cultural', 'Sporting']:\n        s = (calendar['event_type_1']==item)|(calendar['event_type_2']==item)\n        calendar.loc[s, item + '_events'] = 1\n\n    for item in ['Religous_events', 'National_events', 'Cultural_events', 'Sporting_events']:   \n        s = calendar[item].isnull()==True\n        calendar.loc[s, item]=0\n\n\n    # Weekend\n    s = calendar['weekday'].isin(['Saturday', 'Sunday'])\n    calendar.loc[s, 'weekend'] = 1\n    s = calendar['weekend'].isnull()==True\n    calendar.loc[s, 'weekend'] = 0\n\n\n\n    # First day of the month\n    s = calendar['date'].dt.day == 1\n    calendar.loc[s, 'first_day_of_month'] = 1\n    s = calendar['first_day_of_month'].isnull()==True\n    calendar.loc[s, 'first_day_of_month'] = 0\n\n\n    # Seasons\n    for item1, item2 in zip([[12, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]], \n                             ['winter', 'spring', 'summer', 'fall']):\n        s = calendar['date'].dt.month.isin(item1)\n        calendar.loc[s, item2] = 1\n        s = calendar[item2].isnull()==True\n        calendar.loc[s, item2] = 0\n\n    nba_finals_dates = [\n        '2011-05-31', '2011-06-02', '2011-06-05', '2011-06-07',\n        '2011-06-09', '2011-06-12', '2012-06-12', '2012-06-14', \n        '2012-06-17', '2012-06-19','2012-06-21',  '2013-06-06', \n        '2013-06-09', '2013-06-11', '2013-06-13', '2013-06-16', \n        '2013-06-18', '2013-06-20', '2014-06-05', '2014-06-08', \n        '2014-06-10', '2014-06-12', '2014-06-15', '2015-06-04', \n        '2015-06-07', '2015-06-09', '2015-06-11', '2015-06-14', \n        '2015-06-16', '2016-06-02', '2016-06-05', '2016-06-08', \n        '2016-06-10', '2016-06-13', '2016-06-16', '2016-06-19']\n\n    \n    for item in nba_finals_dates:\n        s = calendar['date']==item\n        calendar.loc[s, 'nba_final'] = 1\n\n    s = calendar['nba_final'].isnull()==True\n    calendar.loc[s, 'nba_final'] = 0    \n\n\n\n    # Fourier terms\n\n    calendar['date_todatetime'] = pd.to_datetime(calendar['date'])\n    calendar['dayofyear'] = calendar['date_todatetime'].dt.dayofyear\n\n    calendar['sin365'] = np.sin(2 * np.pi * calendar['dayofyear']  / 365.25)\n    calendar['cos365'] = np.cos(2 * np.pi * calendar['dayofyear']  / 365.25)\n    calendar['sin365_2'] = np.sin(4 * np.pi * calendar['dayofyear']  / 365.25)\n    calendar['cos365_2'] = np.cos(4 * np.pi * calendar['dayofyear'] / 365.25)\n\n    calendar['week_sin365'] = np.sin(2 * np.pi * calendar['dayofyear']  / 7)\n    calendar['week_cos365'] = np.cos(2 * np.pi * calendar['dayofyear']  / 7)\n    calendar['week_sin365_2'] = np.sin(4 * np.pi * calendar['dayofyear']  / 7)\n    calendar['week_cos365_2'] = np.cos(4 * np.pi * calendar['dayofyear'] / 7)\n\n    calendar['month_sin365'] = np.sin(2 * np.pi * calendar['dayofyear']  / 28)\n    calendar['month_cos365'] = np.cos(2 * np.pi * calendar['dayofyear']  / 28)\n    calendar['month_sin365_2'] = np.sin(4 * np.pi * calendar['dayofyear']  / 28)\n    calendar['month_cos365_2'] = np.cos(4 * np.pi * calendar['dayofyear'] / 28)\n    \n    \n\n    # Moon phases\n    calendar['moon'] = calendar.date.apply(get_moon_phase)\n    \n    print('End processing calendar ... ' + str(datetime.now()))\n    \n    \n\n    \n    \n\ndef add_subsample(input_day, st_day_train, st_day_test, end_day_test, cv_size):\n    \n    # Train, Validation and Test subsample should not overlap in this case!\n    # Train = 1\n    # Validation = 2\n    # Test = 3\n    \n    curr = int(input_day.split('_')[1])\n    \n    \n    \n    end_day_validation = st_day_test - 1\n    st_day_validation = end_day_validation - cv_size + 1\n    \n    end_day_train = st_day_validation - 1    \n\n    #print(st_day_train)\n    #print(end_day_train)\n    #print(st_day_validation)\n    #print(end_day_validation)   \n    #print(st_day_test)\n    #print(end_day_test)  \n    \n\n    if curr >= st_day_train and curr <= end_day_train:\n        res = 1\n    elif curr >= st_day_validation and curr <= end_day_validation:\n        res = 2\n    elif curr >= st_day_test and curr <= end_day_test:\n        res = 3\n        \n    return res\n\n\ndef lgbm_val(X_train, X_val, y_train, y_val, cat_features):\n    # create dataset\n    train = lgb.Dataset(X_train, label = y_train, categorical_feature=cat_features)\n    valid = lgb.Dataset(X_val, label = y_val, categorical_feature=cat_features)\n    \n    \n    # parameter setting\n    params1 = {\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        'objective': 'regression',\n        'n_jobs': -1,\n        'seed': 236,\n        'learning_rate': 0.1,\n        'bagging_fraction': 0.75,\n        'bagging_freq': 10, \n        'colsample_bytree': 0.75}\n    \n    \n    params2 = {\n        'metric': 'rmse',\n        'objective': 'poisson',\n        'n_jobs': -1,\n        'seed': 20,\n        'learning_rate': 0.1,\n        'alpha': 0.1,\n        'lambda': 0.1,\n        'bagging_fraction': 0.66,\n        'bagging_freq': 2, \n        'colsample_bytree': 0.77}\n    \n    \n    params3 = {\n        'boosting_type': 'gbdt',\n        'objective': 'tweedie',\n        'tweedie_variance_power': 1.1,\n        'metric': 'rmse',\n        'subsample': 0.5,\n        'subsample_freq': 1,\n        'learning_rate': 0.03,\n        'num_leaves': 2**11-1,\n        'min_data_in_leaf': 2**12-1,\n        'feature_fraction': 0.5,\n        'max_bin': 100,\n        'n_estimators': 1400,\n        'boost_from_average': False,\n        'verbose': -1} \n    \n    \n    params = params1\n    \n    \n    # fitting\n    lgbm = lgb.train(params, \n                    train,\n                    num_boost_round = 2500,\n                    valid_sets = [valid], \n                    verbose_eval = False)\n    \n\n    pred_train = lgbm.predict(X_train)\n    pred_val = lgbm.predict(X_val)\n    \n    return pred_train, pred_val, lgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_and_create_submission():\n    \n    \n    print('Start processing and creating submission file ... ' + str(datetime.now()))\n    \n    if full_run:\n        ss = ['a']\n    elif partial_run:\n        ss = sales_train_validation[f_name_for_mask].value_counts().index.to_list()\n\n\n\n    cnt = 0\n\n    # Submission file template\n    submission_df = pd.DataFrame(data=None, columns=sample_submission.columns, index=sample_submission.index) \n    submission_df.drop(submission_df.index[0:submission_df.shape[0]], axis=0, inplace=True)\n\n\n\n    for curr_item in ss:\n\n        cnt += 1\n        curr_part = cnt\n\n\n        # df of structure: goods in rows, days d_1..d1913 in columns + hierarchy\n        if full_run:\n            input_df = sales_train_validation.loc[:, :]\n        elif partial_run:\n            mask = sales_train_validation[f_name_for_mask]==curr_item\n            input_df = sales_train_validation.loc[mask, :]\n\n\n\n\n        # Adding empty columns for future value that we need to predict\n        col_to_add = ['d_'+str(item) for item in range(f_pred_day, l_pred_day+1)]\n        input_df = input_df.reindex(columns=[*input_df.columns.tolist(), *col_to_add], fill_value=np.nan)\n\n\n\n\n        # Store the hierarchy\n        cols_hierarchy = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n        input_df_hierarchy = input_df[['id'] + cols_hierarchy]\n\n\n        # Deleting the hierarchy columns\n        days_to_delete = ['d_'+str(item) for item in range(1, st_day)]\n        cols_to_delete = days_to_delete + cols_hierarchy \n        melted_df = input_df.drop(cols_to_delete, axis=1) \n\n        # Melting\n        melted_df = melted_df.melt(id_vars='id', var_name='d', value_name='Sales')\n\n        melted_df = reduce_mem_usage(melted_df)\n\n\n\n        # Place the hierarchy ('id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id') back\n        melted_df = melted_df.merge(input_df_hierarchy, how='inner', on='id')\n\n\n        # Adding subsample (1=train, 2=test(for validation), 3=to predict)\n        add_subsample_v = np.vectorize(add_subsample)\n\n        melted_df['subsample'] = add_subsample_v(melted_df['d'], st_day, f_pred_day, l_pred_day, cv_size)\n\n\n\n\n        calendar2 = calendar.copy()\n        cols_to_delete = ['date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', \n                          'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'date_todatetime', 'dayofyear']\n        calendar2 = calendar2.drop(cols_to_delete, axis=1)\n\n        melted_df = melted_df.merge(calendar2, how='inner', on='d')\n\n\n        # Adding sell prices\n        melted_df = melted_df.merge(calendar_d_wm, on='d', how='inner')\n\n        melted_df = melted_df.merge(sell_prices_d_wm, how='left', left_on=['id', 'wm_yr_wk'], right_on=['id', 'wm_yr_wk'])\n\n        melted_df['sell_price'] = melted_df.groupby(['id'])['sell_price'].transform(lambda x: x.fillna(method='bfill'))\n\n\n\n\n\n\n        # Thanks to Ragnar123 for the ideas of some baseline features\n        melted_df['lag_price_t1'] = melted_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n        melted_df['price_change_t1'] = (melted_df['lag_price_t1'] - melted_df['sell_price']) / (melted_df['lag_price_t1'])\n        melted_df['rolling_price_max_t365'] = melted_df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n        melted_df['price_change_t365'] = (melted_df['rolling_price_max_t365'] - melted_df['sell_price']) / (melted_df['rolling_price_max_t365'])\n        melted_df['rolling_price_std_t7'] = melted_df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n        melted_df['rolling_price_std_t30'] = melted_df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n        melted_df.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n\n\n\n\n        for val in [28, 29, 30]:\n            melted_df[f\"shift_t{val}\"] = melted_df.groupby([\"id\"])[\"Sales\"].transform(lambda x: x.shift(val))\n        for val in [7, 30, 60, 90, 180]:\n            melted_df[f\"rolling_std_t{val}\"] = melted_df.groupby([\"id\"])[\"Sales\"].transform(lambda x: x.shift(28).rolling(val).std())\n        for val in [7, 30, 60, 90, 180]:\n            melted_df[f\"rolling_mean_t{val}\"] = melted_df.groupby([\"id\"])[\"Sales\"].transform(lambda x: x.shift(28).rolling(val).mean())\n\n        melted_df[\"rolling_skew_t30\"] = melted_df.groupby([\"id\"])[\"Sales\"].transform( lambda x: x.shift(28).rolling(30).skew())\n        melted_df[\"rolling_kurt_t30\"] = melted_df.groupby([\"id\"])[\"Sales\"].transform(lambda x: x.shift(28).rolling(30).kurt())\n\n\n\n\n\n\n        # Deleting NaN values after applying 365 - the largest lag\n        mask = melted_df['price_change_t365'].isnull()==False\n        melted_df = melted_df.loc[mask, :]\n        melted_df = melted_df.reset_index()\n\n\n\n\n        # Processing categorical features\n        cols_to_encode = ['dept_id', 'cat_id', 'store_id']\n\n        cols_categorical_other = ['snap_CA', 'snap_TX', 'snap_WI', 'black_friday',\n            'Ramadan', 'Religous_events', 'National_events',\n            'Cultural_events', 'Sporting_events', 'weekend',\n            'first_day_of_month', 'winter',\n            'spring', 'summer', 'fall', 'nba_final', 'moon']\n\n        for item in cols_to_encode:\n            melted_df[item] = melted_df[item].astype('category')\n            melted_df[item] = melted_df[item].cat.codes\n            melted_df[item] = melted_df[item].astype('category')\n\n        for item in cols_categorical_other:\n            melted_df[item] = melted_df[item].astype('category')\n\n        \n\n        melted_df = reduce_mem_usage(melted_df)\n\n\n\n        # Splitting to Train, Test, and a sabsample for predictions\n\n        cols_to_delete = ['Sales', 'index', 'id', 'd', 'item_id', 'state_id', 'subsample', 'wm_yr_wk']\n\n        mask = melted_df['subsample']==1\n        X_train = melted_df.loc[mask, :]\n        y_train = X_train['Sales']\n\n        X_train = X_train.drop(cols_to_delete, axis=1)\n\n\n\n        mask = melted_df['subsample']==2\n        X_validation = melted_df.loc[mask, :]\n        y_validation = X_validation['Sales']\n\n        X_validation = X_validation.drop(cols_to_delete, axis=1)\n\n\n\n        mask = melted_df['subsample']==3\n        X_test = melted_df.loc[mask, :]\n        df_prediction_long = X_test[['id', 'd']]\n        X_test = X_test.drop(cols_to_delete, axis=1)\n\n\n        \n        del melted_df\n\n\n\n\n\n        # Training the model\n        # cols_to_encode = list of categorical features declared above\n\n\n\n        a, b, lgbm_model = lgbm_val(X_train, X_validation, y_train, y_validation, cols_to_encode + cols_categorical_other)\n\n\n        # Making predictions\n        predictions = lgbm_model.predict(X_test)\n\n        df_prediction_long['Sales'] = predictions\n\n        df_prediction_pivot = df_prediction_long.pivot(index='id', columns='d')\n\n\n\n        # Pivoting predictions and copying to .csv\n        df_prediction_pivot = pd.DataFrame(df_prediction_pivot.to_records())\n\n        cols=['id'] + ['F'+str(item) for item in range(1, 29)]\n        df_prediction_pivot.columns = cols\n\n        df_prediction_pivot = df_prediction_pivot.reset_index()\n        df_prediction_pivot = df_prediction_pivot.drop(['index'], axis=1)\n\n\n\n        df_prediction_eval = df_prediction_pivot.copy()\n\n        # Just copying the same values for d_1942...d_1968, replacing 'validation'->'evaluation' in labels\n        \n        \n        \n        # Creating a dataframe with validation data\n        i = ['d_' + str(item) for item in range(1914, 1942)]\n\n        sub_validation = sales_train_validation[i]\n\n        i = ['F' + str(item) for item in range(1, 29)]\n\n        sub_validation.columns = i\n\n        sub_validation_ids = sales_train_validation[['id']]\n\n        sub_validation = pd.concat([sub_validation_ids, sub_validation], axis=1)\n\n        sub_validation['id'] = sub_validation['id'].str.replace('evaluation', 'validation')\n        \n        \n        \n        \n        current_submission_part = pd.concat([df_prediction_pivot, sub_validation])\n\n\n        # Appending to the previous dataframe (in case of partial_run)\n        submission_df = pd.concat([submission_df, current_submission_part], ignore_index=True)\n\n    print('End processing and creating submission file ... ' + str(datetime.now()))\n    \n    \n    return submission_df\n\n\n    \n    \ndef load_dataframes():\n    \n    print('Start loading dataframes ... ' + str(datetime.now()))\n    \n    calendar_path = folder_path + 'calendar.csv'\n    sales_train_validation_path = folder_path + 'sales_train_evaluation.csv'\n    sample_submission_path = folder_path + 'sample_submission.csv'\n    sell_prices_path = folder_path + 'sell_prices.csv'   \n\n\n    # files to dataframes\n    calendar = pd.read_csv(calendar_path, sep=',')\n    sales_train_validation = pd.read_csv(sales_train_validation_path, sep=',').pipe(reduce_mem_usage)\n\n\n\n\n    sample_submission = pd.read_csv(sample_submission_path, sep=',')\n    sell_prices = pd.read_csv(sell_prices_path, sep=',').pipe(reduce_mem_usage)\n\n\n\n    # Concatenating 'id' in the 'sell_prices' table\n    sell_prices['id'] = sell_prices['item_id'] + '_' + sell_prices['store_id']\n    #sell_prices['id'] = sell_prices['id'].str.replace('validation', 'evaluation')\n    \n    \n    \n    # Only 2 fields from main calendar\n    calendar_d_wm = calendar[['d', 'wm_yr_wk']]\n\n\n\n    sell_prices_d_wm = sell_prices[['id', 'sell_price', 'wm_yr_wk']]\n    sell_prices_d_wm['id'] = sell_prices_d_wm['id'].map('{}_evaluation'.format)\n    \n    print('End loading dataframes ... ' + str(datetime.now()))\n    \n    return calendar, sales_train_validation, sample_submission, sell_prices, calendar_d_wm, sell_prices_d_wm ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Main\n# Some constants\n\nfolder_path = '/kaggle/input/m5-forecasting-accuracy/'\n\nst_day = 1377\nn_to_predict = 28\ncv_size = 28\nf_pred_day = 1942\nl_pred_day = 1969\nf_name_for_mask = 'cat_id'\n\n# Only full_run=True is possible for evaluation part\nfull_run = True\npartial_run = False\n\noutfile_name = 'submission.csv'\n\n\n# Loading files to dataframes (and small processing)\ncalendar, sales_train_validation, sample_submission, sell_prices, calendar_d_wm, sell_prices_d_wm  = load_dataframes()\n\n# Processing calendar\nprocess_calendar()\n\n    \n\n\n# Two checks in case of a manual mistake\nif full_run and partial_run:\n    partial_run = False\nif not(full_run or partial_run):\n    full_run = True\n\n    \n\n    \n\n\nsubmission_df = process_and_create_submission()\n\n# Saving the result file to .csv   \nsubmission_df.to_csv(outfile_name, sep=',', index=False)\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}