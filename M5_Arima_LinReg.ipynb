{"cells":[{"metadata":{"_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"\n# Code recorded by Artem Remezov\n\n# One of my solutions for 'M5 Forecasting - Accuracy' competition\n# (https://www.kaggle.com/c/m5-forecasting-accuracy)\n# based on Linear Regression and auto-Arima models\n\n\n\n\n!pip install pmdarima\n\nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom datetime import date, timedelta\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport itertools\nimport matplotlib.pyplot as plt\nimport os\nimport matplotlib\nfrom typing import Union\nfrom tqdm.auto import tqdm as tqdm\nfrom sklearn.linear_model import LinearRegression\nimport pmdarima as pm\nfrom math import isnan as math_isnan\nimport math, decimal\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n\n# Receiving a dataframe, calculating correlations,\n# appending correlations with absolute value higher than corr_benchmark\n# to the dictionary\ndef correlations_to_dict(input_df, input_dict, corr_benchmark):\n    \n    \n    cols_to_delete = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n    labels = list(df_for_corr['id'])\n    input_df.drop(cols_to_delete, axis=1, inplace=True)\n    input_df = input_df.transpose()\n    input_df.columns = labels\n    test_corr = input_df.corr()\n    \n    \n    for i, row in test_corr.iterrows():\n        for j, column in row.iteritems():\n            if not(math_isnan(column)):\n                if abs(column) >= corr_benchmark and i != j:\n                    #print(i, '...', j, '...', column)\n                    if i in input_dict:\n                        t = input_dict[i]\n                        t += [j]\n                        input_dict[i] = t\n                    else:\n                        input_dict[i] = [j]\n\n                        \n# Returns a dataframe with all prices for an item\ndef get_s_prices(t_item):\n\n    # Get min and max indexes from the sorted dataframe in order to use .loc method and save time\n    st_pos = items_and_prices['item'].values.searchsorted(t_item, side='left')\n    end_pos = items_and_prices['item'].values.searchsorted(t_item, side='right') - 1\n    item_calendar = items_and_prices.loc[st_pos:end_pos, :]\n\n    \n    d1 = date(2011,1,29)\n    d2 = date(2016,6,19)\n\n    # Creating a list containing all of the dates\n    dd = [d1 + timedelta(days=x) for x in range((d2-d1).days + 1)]\n\n    df_dates = pd.DataFrame(dd)\n    df_dates.columns = ['date']\n    df_dates['date'] =  pd.to_datetime(df_dates['date'], format='%Y-%m-%d')\n    \n    s_prices = df_dates.merge(item_calendar, how='left', on='date')['sell_price']\n    \n    return s_prices                       \n                        \n                        \n# Credits to: https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/154776                        \ndef get_moon_phase(d):  # 0=new, 4=full; 4 days/phase\n    dec = decimal.Decimal\n    diff = d - datetime(2001, 1, 1)\n    days = dec(diff.days) + (dec(diff.seconds) / dec(86400))\n    lunations = dec(\"0.20439731\") + (days * dec(\"0.03386319269\"))\n    phase_index = math.floor((lunations % dec(1) * dec(8)) + dec('0.5'))\n    return int(phase_index) & 7                        \n\n# Returns rollings with a given shift for train and test splits\ndef shifted_df_for_prediction(input_s, t_shift, window_size, n_to_predict, train_days):\n \n    input_df2 = pd.DataFrame(input_s)\n    input_df2.reset_index(inplace=True)\n    input_df2.drop(['index'], axis=1, inplace=True)\n    \n    #print(input_df2)\n    \n    window = input_df2.rolling(window=window_size)\n    shift_df = pd.concat([window.std(), window.min(), window.max(), window.mean()], axis=1)\n    \n    st = shift_df.shape[0] - t_shift\n    en = st + n_to_predict - 1\n    \n    return shift_df[-train_days:], shift_df.loc[st:en, :]\n\n\n# Adds ratio previous/ current Sale\ndef add_ratio_to_prices(input_s):\n    \n    input_df = pd.DataFrame(input_s)\n    \n    input_df.columns = ['sales']\n    input_df['shift_1'] = input_df['sales'].shift(1)\n    \n    # replacing first NaN value with 1\n    input_df.loc[0, 'shift_1'] = 1\n    input_df['ratio'] = input_df['shift_1']/input_df['sales']\n    \n    return input_df\n\n\n# Creating calendar features\ndef process_calendar():\n\n    # Black Friday (https://www.timeanddate.com/holidays/us/black-friday)\n    black_friday_dates = ['2011-11-25', '2012-11-23', '2013-11-29', '2014-11-28', '2015-11-27']\n\n    for item in black_friday_dates:\n        s = calendar['date']==item\n        calendar.loc[s, 'black_friday'] = 1\n\n    s = calendar['black_friday'].isnull()==True\n    calendar.loc[s, 'black_friday'] = 0  \n\n\n\n\n    # Dates of Ramadan\n    Ramadan_dates = [['2011-08-01', '2011-08-30'], \n                     ['2012-07-20', '2012-08-18'], \n                     ['2013-07-09', '2013-08-07'], \n                     ['2014-06-29', '2014-07-27'], \n                     ['2015-06-18', '2015-07-17'], \n                     ['2016-06-07', '2016-07-06']]\n\n    for item in Ramadan_dates:\n        s = (calendar['date'] >= item[0]) & (calendar['date'] <= item[1])\n        calendar.loc[s, 'Ramadan'] = 1\n\n    s = calendar['Ramadan'].isnull()==True\n    calendar.loc[s, 'Ramadan'] = 0\n\n\n\n    # Processing default fields from the calendar\n    for item in ['Religous', 'National', 'Cultural', 'Sporting']:\n        s = (calendar['event_type_1']==item)|(calendar['event_type_2']==item)\n        calendar.loc[s, item + '_events'] = 1\n\n    for item in ['Religous_events', 'National_events', 'Cultural_events', 'Sporting_events']:   \n        s = calendar[item].isnull()==True\n        calendar.loc[s, item]=0\n\n\n\n\n    # Weekend\n    s = calendar['weekday'].isin(['Saturday', 'Sunday'])\n    calendar.loc[s, 'weekend'] = 1\n    s = calendar['weekend'].isnull()==True\n    calendar.loc[s, 'weekend'] = 0\n\n\n\n\n\n\n    # First day of the month\n    s = calendar['date'].dt.day == 1\n    calendar.loc[s, 'first_day_of_month'] = 1\n    s = calendar['first_day_of_month'].isnull()==True\n    calendar.loc[s, 'first_day_of_month'] = 0\n\n\n    # Seasons\n    for item1, item2 in zip([[12, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]], \n                             ['winter', 'spring', 'summer', 'fall']):\n        s = calendar['date'].dt.month.isin(item1)\n        calendar.loc[s, item2] = 1\n        s = calendar[item2].isnull()==True\n        calendar.loc[s, item2] = 0\n\n\n\n    nba_finals_dates = [\n        '2011-05-31', '2011-06-02', '2011-06-05', '2011-06-07',\n        '2011-06-09', '2011-06-12', '2012-06-12', '2012-06-14', \n        '2012-06-17', '2012-06-19','2012-06-21',  '2013-06-06', \n        '2013-06-09', '2013-06-11', '2013-06-13', '2013-06-16', \n        '2013-06-18', '2013-06-20', '2014-06-05', '2014-06-08', \n        '2014-06-10', '2014-06-12', '2014-06-15', '2015-06-04', \n        '2015-06-07', '2015-06-09', '2015-06-11', '2015-06-14', \n        '2015-06-16', '2016-06-02', '2016-06-05', '2016-06-08', \n        '2016-06-10', '2016-06-13', '2016-06-16', '2016-06-19']\n\n\n    for item in nba_finals_dates:\n        s = calendar['date']==item\n        calendar.loc[s, 'nba_final'] = 1\n\n    s = calendar['nba_final'].isnull()==True\n    calendar.loc[s, 'nba_final'] = 0    \n\n\n\n\n    # Fourier terms\n\n    calendar['date_todatetime'] = pd.to_datetime(calendar['date'])\n    calendar['dayofyear'] = calendar['date_todatetime'].dt.dayofyear\n\n\n    calendar['sin365'] = np.sin(2 * np.pi * calendar['dayofyear']  / 365.25)\n    calendar['cos365'] = np.cos(2 * np.pi * calendar['dayofyear']  / 365.25)\n    calendar['sin365_2'] = np.sin(4 * np.pi * calendar['dayofyear']  / 365.25)\n    calendar['cos365_2'] = np.cos(4 * np.pi * calendar['dayofyear'] / 365.25)\n\n    \n    calendar['week_sin365'] = np.sin(2 * np.pi * calendar['dayofyear']  / 7)\n    calendar['week_cos365'] = np.cos(2 * np.pi * calendar['dayofyear']  / 7)\n    calendar['week_sin365_2'] = np.sin(4 * np.pi * calendar['dayofyear']  / 7)\n    calendar['week_cos365_2'] = np.cos(4 * np.pi * calendar['dayofyear'] / 7)\n\n    calendar['month_sin365'] = np.sin(2 * np.pi * calendar['dayofyear']  / 30)\n    calendar['month_cos365'] = np.cos(2 * np.pi * calendar['dayofyear']  / 30)\n    calendar['month_sin365_2'] = np.sin(4 * np.pi * calendar['dayofyear']  / 30)\n    calendar['month_cos365_2'] = np.cos(4 * np.pi * calendar['dayofyear'] / 30)\n\n\n\n    # Moon phases\n    calendar['moon'] = calendar.date.apply(get_moon_phase)\n\n    \n# Linear Regression + auto Arima (optional) on residuals from linear regression\ndef model_results_to_dfs(dict_corr, input_df, res_df, exog_s):\n    \n    b_test = False\n    already_predicted = {}\n    cnt_used_corr = 0\n\n    \n\n\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    \n    st = total - n_train_days\n   \n    st_pred = total - st + 1\n        \n    end_pred = st_pred + to_predict - 1\n    \n    \n    \n    if b_test:\n        print('st: ', st)\n        print('st_pred: ', st_pred)\n        print('end_pred: ', end_pred)\n        \n        \n    # exog_s = DataFrame\n    exog_to_fit = exog_s.loc[0:total-1]\n    if b_test:\n        print('exog_to_fit.shape: ', exog_to_fit.shape)\n    e = exog_to_fit.loc[st:].astype('float64')      \n    e = np.asarray(e)\n    if b_test:\n        print('e.shape: ', e.shape)\n    \n    \n    ep = exog_s.loc[exog_s.shape[0] - to_predict - n_train_days:exog_s.shape[0] - to_predict - 1].astype('float64')\n    ep = np.asarray(ep)\n    \n    \n    epf = exog_s.loc[exog_s.shape[0] - to_predict:].astype('float64')\n    epf = np.asarray(epf)\n    \n    \n    if b_test:\n        print('ep.shape: ', ep.shape)\n        print('epf.shape: ', epf.shape)\n    \n    \n    \n    print('start:')\n    print(datetime.now())\n    \n    cnt = 0\n    \n    res_df1 = res_df.copy()\n    #res_df2 = res_df.copy()\n    \n    \n    # Iterating over the dataframe rows\n    for i, row in input_df.iterrows():\n        \n        cnt += 1\n        tmp = input_df.loc[i, :]\n        id = tmp['id']\n\n        s = tmp.drop(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])\n        lags_train1, lags_test1 = shifted_df_for_prediction(s, 365, 7, to_predict, n_train_days)\n        lags_train2, lags_test2 = shifted_df_for_prediction(s, 28, 7, to_predict, n_train_days)\n        \n        s = np.asarray(s[st:].astype('float64'))\n        \n        # Applying sell prices if necessary\n        if use_prices:\n\n            cp = get_s_prices(id.replace('_evaluation', ''))\n            cp.fillna(method='bfill', inplace=True)\n            cp = np.asarray(cp).reshape(len(cp),1)\n\n            cp = add_ratio_to_prices(cp)\n            \n            cp = np.asarray(cp)\n            \n            scaler = scaler.fit(cp)\n            cp = scaler.transform(cp)\n\n            cp_actual = cp[st:total]\n            cp_predict = cp[-to_predict:]\n\n     \n\n        if sum(s)==0:\n            predictions = [0 for item in range(to_predict)]\n        else:\n            \n\n            ep2 = np.copy(ep)\n            epf2 = np.copy(epf)\n            ep2 = np.concatenate([ep, lags_train1], axis=1)\n            epf2 = np.concatenate([epf, lags_test1], axis=1)            \n\n\n            \n            ep2 = np.concatenate([ep2, lags_train2], axis=1)\n            epf2 = np.concatenate([epf2, lags_test2], axis=1)  \n            \n            if use_prices:\n                ep2 = np.concatenate([ep, cp_actual], axis=1)\n                epf2 = np.concatenate([epf, cp_predict], axis=1)            \n            \n            \n            \n            if use_correlations: \n\n                if id in dict_corr:\n                    t = dict_corr[id]\n                    if not(t is None):\n                        for item in t:\n                            if item in already_predicted:\n\n                                t1 = already_predicted[item][0]\n                                t1 = t1.reshape(len(t1), 1)\n                                ep2 = np.concatenate([ep2, t1], axis=1)\n\n\n                                t2 = already_predicted[item][1]\n                                t2 = t2.reshape(len(t2), 1)\n                                epf2 = np.concatenate([epf2, t2], axis=1)\n                                cnt_used_corr += 1\n                            \n            linreg = LinearRegression().fit(ep2, s)\n            \n            \n            reg_predicted = linreg.predict(epf2)\n            \n            if use_Arima:\n\n                predicted_for_actual = linreg.predict(ep2)\n            \n                resid = s - predicted_for_actual\n           \n                autoarima = pm.auto_arima(resid[-n_train_days_Arima:], start_p=0, d=0, start_q=0,\n                                          max_p=2, max_d=1, max_q=2,\n                                          start_P=0, D=0, start_Q=0,\n                                          max_P=2, max_D=1, max_Q=2,\n                                          stepwise=True, error_action='ignore', seasonal=True, m=7)\n           \n                results_autoarima = autoarima.fit(resid[-n_train_days_Arima:])\n            \n                predicted_autoarima = results_autoarima.predict(n_periods=to_predict)\n            \n            \n                predictions = list(reg_predicted + predicted_autoarima)\n            else:\n                predictions = list(reg_predicted)\n                \n                \n            if use_correlations:\n                already_predicted[id] = [s, np.array(predictions)]\n                        \n            predictions = [0 if item < 0 else item for item in predictions]            \n            \n\n\n        t1 = predictions[:]        \n        \n        list_out1 = [id] + t1             \n        \n        series_out1 = pd.Series(list_out1, index=sample_submission.columns) \n                \n        res_df1 = res_df1.append(series_out1, ignore_index=True)\n        \n        \n        if cnt % 1000 == 0:\n            print('Rows processed: ', str(cnt))\n            #print(datetime.now())\n            \n            \n    if use_correlations:  \n        print('n of outer correlations used: ', str(cnt_used_corr))\n    \n    \n    print('end:')\n    print(datetime.now())\n    return res_df1\n    \n\ndef load_dataframes():\n    calendar_path = folder_path + 'calendar.csv'\n    sales_train_validation_path = folder_path + 'sales_train_evaluation.csv'\n    sample_submission_path = folder_path + 'sample_submission.csv'\n    sell_prices_path = folder_path + 'sell_prices.csv'   \n\n\n    # Uploading files to dataframes\n    calendar = pd.read_csv(calendar_path, sep=',')\n    sales_train_validation = pd.read_csv(sales_train_validation_path, sep=',')\n    sample_submission = pd.read_csv(sample_submission_path, sep=',')\n    sell_prices = pd.read_csv(sell_prices_path, sep=',')\n\n\n    # Converting 'date' to date format\n    calendar['date'] =  pd.to_datetime(calendar['date'], format='%Y-%m-%d')\n\n    return calendar, sales_train_validation, sample_submission, sell_prices\n\n\n### Main ###\nfolder_path = '/kaggle/input/m5-forecasting-accuracy/'\n\n\ntotal = 1941\nn_train_days = 273\nn_train_days_Arima = 70\nto_predict = 28\n\ncutoff_correlation = 0.5\n\n\nfull_run = True\npartial_run = False\ncurr_part = 1\n\n\nuse_correlations = False\nuse_prices = True\nuse_Arima = False\n\n\nif partial_run:    \n    corr_by_field = 'state_id'\n    if curr_part==1:\n        t_field_value='CA'\n    elif curr_part==2:\n        t_field_value='TX'\n    elif curr_part==3:\n        t_field_value='WI'\n  \n\n\n\n# Revise mode in case of a mistake\nif full_run and partial_run:\n    partial_run = False\n\nif not(full_run) and not(partial_run):\n    full_run = True\n\nif n_train_days_Arima > n_train_days:\n    n_train_days_Arima = n_train_days\n\n\n# Loading dataframes\ncalendar, sales_train_validation, sample_submission, sell_prices = load_dataframes()\n\n# Applying additional features to the calendar\nprocess_calendar()\n\n# if NA for sell price - next valid price is used\nif use_prices:\n    print('Start processing items_and_prices df: ', datetime.now())\n    sell_prices['item'] = sell_prices['item_id'] + '_' + sell_prices['store_id']\n\n    calendar2 = calendar[['date', 'wm_yr_wk']]\n    items_and_prices = sell_prices.merge(calendar2, how='inner', on='wm_yr_wk')[['item', 'date', 'sell_price']]\n    items_and_prices.sort_values(inplace=True, by='item')\n    items_and_prices.reset_index(inplace=True, drop=True)\n    print('End processing items_and_prices df: ', datetime.now())\n\n\n\n\n# Appending inner correlations to dictionary\n# Splitting into 3 groups (=states: California, Texas, Wisconsin) because a run on the full dataframe may\n# last longer than Kaggle session lifetime\n\ndict_corr = {}\n\nif use_correlations:\n    \n    print('Collecting correlations, start:')\n    print(datetime.now())\n    \n    s = sales_train_validation[corr_by_field]==t_field_value\n    df_for_corr = sales_train_validation.loc[s, :]\n    correlations_to_dict(df_for_corr, dict_corr, cutoff_correlation)   \n    \n    print('Collecting correlations, end:')\n    print(datetime.now())\n\n\n\n\n# Creating submission file template\nres_df = pd.DataFrame(data=None, columns=sample_submission.columns, index=sample_submission.index) \nres_df.drop(res_df.index[0:res_df.shape[0]], axis=0, inplace=True)\n\n\n\n\nif partial_run:  \n    if curr_part==1:\n        t1 = sales_train_validation.loc[0:12195, :]\n        #t1 = sales_train_validation.loc[0:1, :]\n        out_file = 'submission1.csv'\n    elif curr_part==2:\n        t1 = sales_train_validation.loc[12196:21342, :]\n        out_file = 'submission2.csv'\n    elif curr_part==3:\n        t1 = sales_train_validation.loc[21343:30489, :]\n        out_file = 'submission3.csv'\n\n        \nif full_run:\n    out_file = 'submission.csv'\n    t1 = sales_train_validation.loc[:, :]\n\n    \n\n    \n\n# Exogenous variables from the calendar to include into the model\nlist_exog_to_include = ['sin365', 'cos365', 'sin365_2', 'cos365_2',\n                        'week_sin365', 'week_cos365', 'week_sin365_2', 'week_cos365_2', \n                        'month_sin365', 'month_cos365', 'month_sin365_2', 'month_cos365_2',\n                        'snap_CA', 'snap_TX', 'snap_WI',\n                        'black_friday', 'Ramadan', 'Religous_events', 'National_events', \n                        'Cultural_events', 'Sporting_events', 'weekend', 'first_day_of_month',\n                        'winter', 'spring', 'summer', 'fall', 'moon', 'nba_final']\n\nexog_s_to_model = calendar[list_exog_to_include]\n\n\n# Creating a dataframe with validation data\ni = ['d_' + str(item) for item in range(1914, 1942)]\n\nsub_validation = sales_train_validation[i]\n\ni = ['F' + str(item) for item in range(1, 29)]\n\nsub_validation.columns = i\n\nsub_validation_ids = sales_train_validation[['id']]\n\nsub_validation = pd.concat([sub_validation_ids, sub_validation], axis=1)\n\nsub_validation['id'] = sub_validation['id'].str.replace('evaluation', 'validation')\n        \n\n\nres_df1 = model_results_to_dfs(dict_corr, t1, res_df, exog_s_to_model)\n\nsubmission_df = pd.concat([res_df1, sub_validation], ignore_index=True)\n\n\n# Saving the result to .csv\nsubmission_df.to_csv(out_file, sep=',', index=False)\n\n    \n\n\n","execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}